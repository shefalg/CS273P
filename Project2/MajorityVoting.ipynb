{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "import mltools as ml\n",
    "warnings.filterwarnings('ignore')\n",
    "min_max=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_csv('X_train.csv')\n",
    "y=pd.read_csv('Y_train.csv')\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "le=LabelEncoder()\n",
    "for col in X_test.columns.values:\n",
    "    if X_test[col].dtypes=='object':\n",
    "        data=X_train[col].append(X_test[col])\n",
    "        le.fit(data.values)\n",
    "        X_train[col]=le.transform(X_train[col])\n",
    "        X_test[col]=le.transform(X_test[col])\n",
    "\n",
    "# X_train[X_train.dtypes[(X_train.dtypes==\"float64\")|(X_train.dtypes==\"int64\")]\n",
    "#                         .index.values].hist(figsize=[20,20], color='brown', histtype='step')\n",
    "        \n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.read_csv('X_train.csv')\n",
    "y=pd.read_csv('Y_train.csv')\n",
    "kaggleTest =pd.read_csv('X_test.csv')\n",
    "\n",
    "X_traink, X_testk, Y_traink, Y_testk = train_test_split(\n",
    "    X, y, test_size=0.0, random_state=42)\n",
    "\n",
    "\n",
    "for col in X_testk.columns.values:\n",
    "    if X_testk[col].dtypes=='object':\n",
    "        data=X_traink[col].append(X_testk[col])\n",
    "        le.fit(data.values)\n",
    "        X_traink[col]=le.transform(X_traink[col])\n",
    "        X_testk[col]=le.transform(X_testk[col])\n",
    "\n",
    "le=LabelEncoder()\n",
    "for col in kaggleTest.columns.values:\n",
    "    if kaggleTest[col].dtypes=='object':\n",
    "        data=kaggleTest[col].append(X_testk[col])\n",
    "        le.fit(data.values)\n",
    "        kaggleTest[col]=le.transform(kaggleTest[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9237184163584927\n",
      "(15000, 12)\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(penalty='l2',C=.01)\n",
    "clf2 = RandomForestClassifier(max_depth=11,min_samples_leaf=7,max_features=13,n_estimators=850,bootstrap =True)\n",
    "clf3 = AdaBoostClassifier(n_estimators=950, learning_rate=1.1)\n",
    "clf4 = DecisionTreeClassifier(max_depth=9,min_samples_leaf=4)\n",
    "clf5 = KNeighborsClassifier(n_neighbors=21)\n",
    "clf6 = GradientBoostingClassifier(n_estimators=40, learning_rate=0.8,max_depth=3, min_samples_leaf=7,random_state=0)\n",
    "\n",
    "# eclf1 = VotingClassifier(estimators=[\n",
    "#          ('lr', clf1), ('rf', clf2), ('ada', clf3), ('dt',clf4),('knn',clf5),('gb',clf6)], voting='soft')\n",
    "# eclf1 = eclf1.fit(X_train, Y_train)\n",
    "# y_pred = eclf1.predict_proba(X_test)\n",
    "# y_pred=y_pred[:,1]\n",
    "# print(roc_auc_score(Y_test,y_pred))\n",
    "\n",
    "# eclf2 = VotingClassifier(estimators=[\n",
    "#         ('lr', clf1), ('rf', clf2), ('ada', clf3), ('dt',clf4),('knn',clf5),('gb',clf6)],voting='soft')\n",
    "# eclf2 = eclf2.fit(X_train, Y_train)\n",
    "# y_pred = eclf2.predict_proba(X_test)\n",
    "# y_pred=y_pred[:,1]\n",
    "# print(roc_auc_score(Y_test,y_pred))\n",
    "eclf3 = VotingClassifier(estimators=[\n",
    "       ('lr', clf1), ('rf', clf2), ('ada', clf3), ('dt',clf4),('knn',clf5),('gb',clf6)],voting='soft', \n",
    "                         weights=[1,10,50,2,2,90],flatten_transform=True)\n",
    "eclf3 = eclf3.fit(X_train, Y_train)\n",
    "y_pred = eclf3.predict_proba(X_test)\n",
    "y_pred=y_pred[:,1]\n",
    "print(roc_auc_score(Y_test,y_pred))\n",
    "\n",
    "print(eclf3.transform(X_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf3.fit(X_traink,Y_traink)\n",
    "y_pred = eclf3.predict_proba(kaggleTest)\n",
    "y_pred=y_pred[:,1]\n",
    "\n",
    "# Now output a file with the predictions on test data:\n",
    "np.savetxt('MV2.csv',\n",
    "  np.vstack((np.arange(len(y_pred)), y_pred)).T,\n",
    "  '%d, %f',header='ID,Prob1',comments='',delimiter=',');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,)\n",
      "[0.91182746 0.35017698 0.28096627 ... 0.54036533 0.00493418 0.00191273]\n",
      "(6000, 1)\n",
      "0.9196122037958419\n"
     ]
    }
   ],
   "source": [
    "gb= GradientBoostingClassifier(n_estimators=40, learning_rate=0.8,max_depth=2, min_samples_leaf=7,random_state=0)\n",
    "gb= gb.fit(X_train,Y_train)\n",
    "y_pred= gb.predict_proba(X_test)\n",
    "y_pred=y_pred[:,1]\n",
    "print(y_pred.shape)\n",
    "print(y_pred)\n",
    "print(Y_test.shape)\n",
    "print(roc_auc_score(Y_test,y_pred))\n",
    "gb.fit(X_traink,Y_traink)\n",
    "y_pred =gb.predict_proba(kaggleTest)\n",
    "y_pred=y_pred[:,1]\n",
    "# Now output a file with the predictions on test data:\n",
    "np.savetxt('xgb.csv',\n",
    "  np.vstack((np.arange(len(y_pred)), y_pred)).T,\n",
    "  '%d, %f',header='ID,Prob1',comments='',delimiter=',');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "param = {\n",
    " 'n_estimators':[50,100,150,200,300,400,500,600,800,850,900],\n",
    " 'max_depth':[2,3,4,5,6,7,8],\n",
    " 'min_child_weight':[2,3,4,5],\n",
    " 'colsample_bytree':[0.2,0.6,0.8],\n",
    " 'colsample_bylevel':[0.2,0.6,0.8]\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( \n",
    "        objective= \"multi:softmax\", \n",
    "        num_class = 2,\n",
    "        seed=1), \n",
    "    param_grid = param, \n",
    "    scoring=\"roc_auc\",\n",
    "    cv=10,\n",
    "    verbose = 1)\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "# mean_squared_error alternative.\n",
    "\n",
    "gsearch1.fit(X_train, Y_train)\n",
    "gsearch1.bestscore\n",
    "gsearch1.bestparams\n",
    "y_pred = gsearch1.predict_proba(X_test)\n",
    "y_pred=y_pred[:,1]\n",
    "print(roc_auc_score(Y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
